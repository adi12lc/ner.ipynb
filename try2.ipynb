{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv(r\"C:\\Users\\adity\\PycharmProjects\\pythonProject\\git\\learn_for\\diabetes.csv\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Outcome'] = le.fit_transform(data['Outcome'])\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45437282986111116\n"
     ]
    }
   ],
   "source": [
    "class_labels = np.unique(Y) \n",
    "gini = 0 \n",
    "for cls in class_labels:\n",
    "    p_cls = len(Y[Y == cls]) / len(Y) \n",
    "    gini += p_cls ** 2 \n",
    "print(1 - gini) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gini_index(y):\n",
    "    class_labels = np.unique(y) # take unique values from target column\n",
    "    gini = 0 # initialize gini value with 0\n",
    "    for cls in class_labels:\n",
    "        p_cls = len(y[y == cls]) / len(y) # draw values from class labels and get the probablity\n",
    "        gini += p_cls ** 2 # using gini index formula and update after every iteration\n",
    "    return 1 - gini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_index': 0, 'threshold': 6.0, 'dataset_left': array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
      "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
      "       [  1.   ,  89.   ,  66.   , ...,   0.167,  21.   ,   0.   ],\n",
      "       ...,\n",
      "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
      "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
      "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]]), 'dataset_right': array([[8.00e+00, 1.83e+02, 6.40e+01, ..., 6.72e-01, 3.20e+01, 1.00e+00],\n",
      "       [1.00e+01, 1.15e+02, 0.00e+00, ..., 1.34e-01, 2.90e+01, 0.00e+00],\n",
      "       [8.00e+00, 1.25e+02, 9.60e+01, ..., 2.32e-01, 5.40e+01, 1.00e+00],\n",
      "       ...,\n",
      "       [9.00e+00, 1.70e+02, 7.40e+01, ..., 4.03e-01, 4.30e+01, 1.00e+00],\n",
      "       [9.00e+00, 8.90e+01, 6.20e+01, ..., 1.42e-01, 3.30e+01, 0.00e+00],\n",
      "       [1.00e+01, 1.01e+02, 7.60e+01, ..., 1.71e-01, 6.30e+01, 0.00e+00]]), 'info_gain': 0.025641862239203506}\n"
     ]
    }
   ],
   "source": [
    "dataset = data.to_numpy()\n",
    "X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "n_rows, n_columns = np.shape(X)\n",
    "best_split = {}\n",
    "max_info_gain = -1\n",
    "\n",
    "# loop over all the features\n",
    "for f_idxs in range(n_columns):\n",
    "    feature_values = dataset[:, f_idxs]\n",
    "    possible_thresholds = np.unique(feature_values)\n",
    "    # loop over all the feature values present in the data\n",
    "    for thrs in possible_thresholds:\n",
    "        # get current split\n",
    "        left_idxs = np.array([row for row in dataset if row[f_idxs] <= thrs])\n",
    "        right_idxs = np.array([row for row in dataset if row[f_idxs] > thrs])\n",
    "        # check if childs are not null\n",
    "        if len(left_idxs) > 0 and len(right_idxs) > 0:\n",
    "            y, left_y, right_y = dataset[:, -1], left_idxs[:, -1], right_idxs[:, -1]\n",
    "            # compute information gain\n",
    "            weight_l = len(left_y) / len(y)\n",
    "            weight_r = len(right_y) / len(y)\n",
    "            curr_info_gain = gini_index(y) - (weight_l * gini_index(left_y) + weight_r * gini_index(right_y))\n",
    "            # update the best split if needed\n",
    "            if curr_info_gain > max_info_gain:\n",
    "                best_split[\"feature_index\"] = f_idxs\n",
    "                best_split[\"threshold\"] = thrs\n",
    "                best_split[\"dataset_left\"] = left_idxs\n",
    "                best_split[\"dataset_right\"] = right_idxs\n",
    "                best_split[\"info_gain\"] = curr_info_gain\n",
    "                max_info_gain = curr_info_gain\n",
    "print(best_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_split(dataset, num_features):\n",
    "\n",
    "    best_split = {} #create a dictionary to load best split values\n",
    "    max_info_gain = -1 # initialize info gain value with -1\n",
    "\n",
    "    for feature_index in range(num_features):\n",
    "        feature_values = dataset[:, feature_index]\n",
    "        possible_thresholds = np.unique(feature_values) # draw the unique values from all the features and load it to a variable\n",
    "\n",
    "        for threshold in possible_thresholds:\n",
    "            dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold]) # spliting the left tree and right tree for every to the threshold\n",
    "            dataset_right = np.array([row for row in dataset if row[feature_index] > threshold]) # spliting the left tree and right tree for every to the threshold\n",
    "            if len(dataset_left) > 0 and len(dataset_right) > 0: # check if child nodes are not null\n",
    "                y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1] # load all the dependent values of parent, left and right child into three different variable\n",
    "                weight_l = len(left_y) / len(y) # calculating weighted average left child\n",
    "                weight_r = len(right_y) / len(y) # calculating weighted average right child\n",
    "                curr_info_gain = gini_index(y) - (weight_l * gini_index(left_y) + weight_r * gini_index(right_y)) # obtaining information gain value\n",
    "                # update the best split if current info gain is higher than previous value \n",
    "                if curr_info_gain > max_info_gain: \n",
    "                    best_split[\"feature_index\"] = feature_index\n",
    "                    best_split[\"threshold\"] = threshold\n",
    "                    best_split[\"dataset_left\"] = dataset_left\n",
    "                    best_split[\"dataset_right\"] = dataset_right\n",
    "                    best_split[\"info_gain\"] = curr_info_gain\n",
    "                    max_info_gain = curr_info_gain\n",
    "\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "curr_depth=0\n",
    "min_samples_split=3\n",
    "max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method count of list object at 0x000001B04232E580>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dataset = data.to_numpy()\n",
    "X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "n_rows, n_columns = np.shape(X)\n",
    "\n",
    "if n_rows >= min_samples_split and curr_depth <= max_depth: # checking for stoping criteria\n",
    "    best_split = get_best_split(dataset, n_columns)\n",
    "    if best_split[\"info_gain\"] > 0: \n",
    "        left_subtree = (best_split[\"dataset_left\"], curr_depth + 1)\n",
    "        right_subtree = (best_split[\"dataset_right\"], curr_depth + 1)\n",
    "        print(best_split[\"feature_index\"], best_split[\"threshold\"],left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "\n",
    "Y = list(Y)\n",
    "leaf_value = max(Y, key=Y.count)\n",
    "print(Y.count)\n",
    "print(leaf_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
