{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Outcome'] = le.fit_transform(data['Outcome'])\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45437282986111116\n"
     ]
    }
   ],
   "source": [
    "class_labels = np.unique(Y) \n",
    "gini = 0 \n",
    "for cls in class_labels:\n",
    "    p_cls = len(Y[Y == cls]) / len(Y) \n",
    "    gini += p_cls ** 2 \n",
    "print(1 - gini) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gini_index(y):\n",
    "    class_labels = np.unique(y) # take unique values from target column\n",
    "    gini = 0 # initialize gini value with 0\n",
    "    for cls in class_labels:\n",
    "        p_cls = len(y[y == cls]) / len(y) # draw values from class labels and get the probablity\n",
    "        gini += p_cls ** 2 # using gini index formula and update after every iteration\n",
    "    return 1 - gini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_index': 0, 'threshold': 6.0, 'dataset_left': array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
      "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
      "       [  1.   ,  89.   ,  66.   , ...,   0.167,  21.   ,   0.   ],\n",
      "       ...,\n",
      "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
      "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
      "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]]), 'dataset_right': array([[8.00e+00, 1.83e+02, 6.40e+01, ..., 6.72e-01, 3.20e+01, 1.00e+00],\n",
      "       [1.00e+01, 1.15e+02, 0.00e+00, ..., 1.34e-01, 2.90e+01, 0.00e+00],\n",
      "       [8.00e+00, 1.25e+02, 9.60e+01, ..., 2.32e-01, 5.40e+01, 1.00e+00],\n",
      "       ...,\n",
      "       [9.00e+00, 1.70e+02, 7.40e+01, ..., 4.03e-01, 4.30e+01, 1.00e+00],\n",
      "       [9.00e+00, 8.90e+01, 6.20e+01, ..., 1.42e-01, 3.30e+01, 0.00e+00],\n",
      "       [1.00e+01, 1.01e+02, 7.60e+01, ..., 1.71e-01, 6.30e+01, 0.00e+00]]), 'info_gain': 0.025641862239203506}\n"
     ]
    }
   ],
   "source": [
    "dataset = data.to_numpy()\n",
    "X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "n_rows, n_columns = np.shape(X)\n",
    "best_split = {}\n",
    "max_info_gain = -1\n",
    "\n",
    "# loop over all the features\n",
    "for f_idxs in range(n_columns):\n",
    "    feature_values = dataset[:, f_idxs]\n",
    "    possible_thresholds = np.unique(feature_values)\n",
    "    # loop over all the feature values present in the data\n",
    "    for thrs in possible_thresholds:\n",
    "        # get current split\n",
    "        left_idxs = np.array([row for row in dataset if row[f_idxs] <= thrs])\n",
    "        right_idxs = np.array([row for row in dataset if row[f_idxs] > thrs])\n",
    "        # check if childs are not null\n",
    "        if len(left_idxs) > 0 and len(right_idxs) > 0:\n",
    "            y, left_y, right_y = dataset[:, -1], left_idxs[:, -1], right_idxs[:, -1]\n",
    "            # compute information gain\n",
    "            weight_l = len(left_y) / len(y)\n",
    "            weight_r = len(right_y) / len(y)\n",
    "            curr_info_gain = gini_index(y) - (weight_l * gini_index(left_y) + weight_r * gini_index(right_y))\n",
    "            # update the best split if needed\n",
    "            if curr_info_gain > max_info_gain:\n",
    "                best_split[\"feature_index\"] = f_idxs\n",
    "                best_split[\"threshold\"] = thrs\n",
    "                best_split[\"dataset_left\"] = left_idxs\n",
    "                best_split[\"dataset_right\"] = right_idxs\n",
    "                best_split[\"info_gain\"] = curr_info_gain\n",
    "                max_info_gain = curr_info_gain\n",
    "print(best_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_split(dataset, num_features):\n",
    "\n",
    "    best_split = {} #create a dictionary to load best split values\n",
    "    max_info_gain = -1 # initialize info gain value with -1\n",
    "\n",
    "    for feature_index in range(num_features):\n",
    "        feature_values = dataset[:, feature_index]\n",
    "        possible_thresholds = np.unique(feature_values) # draw the unique values from all the features and load it to a variable\n",
    "\n",
    "        for threshold in possible_thresholds:\n",
    "            dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold]) # spliting the left tree and right tree for every to the threshold\n",
    "            dataset_right = np.array([row for row in dataset if row[feature_index] > threshold]) # spliting the left tree and right tree for every to the threshold\n",
    "            if len(dataset_left) > 0 and len(dataset_right) > 0: # check if child nodes are not null\n",
    "                y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1] # load all the dependent values of parent, left and right child into three different variable\n",
    "                weight_l = len(left_y) / len(y) # calculating weighted average left child\n",
    "                weight_r = len(right_y) / len(y) # calculating weighted average right child\n",
    "                curr_info_gain = gini_index(y) - (weight_l * gini_index(left_y) + weight_r * gini_index(right_y)) # obtaining information gain value\n",
    "                # update the best split if current info gain is higher than previous value \n",
    "                if curr_info_gain > max_info_gain: \n",
    "                    best_split[\"feature_index\"] = feature_index\n",
    "                    best_split[\"threshold\"] = threshold\n",
    "                    best_split[\"dataset_left\"] = dataset_left\n",
    "                    best_split[\"dataset_right\"] = dataset_right\n",
    "                    best_split[\"info_gain\"] = curr_info_gain\n",
    "                    max_info_gain = curr_info_gain\n",
    "\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "curr_depth=0\n",
    "min_samples_split=3\n",
    "max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method count of list object at 0x7f1e622807c0>\n"
     ]
    }
   ],
   "source": [
    "dataset = data.to_numpy()\n",
    "X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "n_rows, n_columns = np.shape(X)\n",
    "\n",
    "if n_rows >= min_samples_split and curr_depth <= max_depth: # checking for stoping criteria\n",
    "    best_split = get_best_split(dataset, n_columns) \n",
    "    if best_split[\"info_gain\"] > 0: \n",
    "        left_subtree = (best_split[\"dataset_left\"], curr_depth + 1) # recur left tree until stopping criteria is met \n",
    "        right_subtree = (best_split[\"dataset_right\"], curr_depth + 1) # recur right tree until stopping criteria is met \n",
    "        print(best_split[\"feature_index\"], best_split[\"threshold\"],left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "\n",
    "Y = list(Y)\n",
    "leaf_value = max(Y, key=Y.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node={}\n",
    "value=None\n",
    "def build_tree(dataset, curr_depth=0):\n",
    "        ''' recursive function to build the tree '''\n",
    "\n",
    "        X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "\n",
    "        # split until stopping conditions are met\n",
    "        if num_samples >= min_samples_split and curr_depth <=max_depth:\n",
    "            # find the best split\n",
    "            best_split = get_best_split(dataset,num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                # recur left nodes\n",
    "                left_subtree = build_tree(best_split[\"dataset_left\"], curr_depth + 1)\n",
    "                # recur right nodes\n",
    "                right_subtree = build_tree(best_split[\"dataset_right\"], curr_depth + 1)\n",
    "                # return decision node\n",
    "                return best_split[\"feature_index\"], best_split[\"threshold\"],left_subtree, right_subtree, best_split[\"info_gain\"]\n",
    "\n",
    "        # compute leaf node\n",
    "        Y = list(Y)\n",
    "        leaf_value = max(Y, key=Y.count)\n",
    "        value=leaf_value      \n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = build_tree(dataset,curr_depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 127.0,\n",
       " (7,\n",
       "  28.0,\n",
       "  (5,\n",
       "   45.3,\n",
       "   (5, 30.9, 0.0, 0.0, 0.009898623023798292),\n",
       "   (1, 115.0, 1.0, 0.0, 0.375),\n",
       "   0.013255334468846858),\n",
       "  (5,\n",
       "   26.2,\n",
       "   (5, 0.0, 1.0, 0.0, 0.09280190362879237),\n",
       "   (1, 99.0, 0.0, 0.0, 0.043906943402416077),\n",
       "   0.03795997591707889),\n",
       "  0.030059990043791396),\n",
       " (5,\n",
       "  29.9,\n",
       "  (1,\n",
       "   145.0,\n",
       "   (4, 130.0, 0.0, 0.0, 0.019886122206169754),\n",
       "   (7, 24.0, 0.0, 1.0, 0.06825543120474004),\n",
       "   0.06726958603183181),\n",
       "  (1,\n",
       "   157.0,\n",
       "   (7, 30.0, 0.0, 1.0, 0.0340159953468081),\n",
       "   (4, 579.0, 1.0, 0.0, 0.01938503147058593),\n",
       "   0.03360638521319054),\n",
       "  0.06566973418273542),\n",
       " 0.08250014459160077)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d4b9e659d6cc1de530d8b1261e1d1a36fd22fd1654ba720ac9cf66df0fbf94f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
